{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "ijmpgYnKYklI",
        "bbFf2-_FphqN",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "hwyV_J3ipUZe",
        "T5CmagL3EC8N",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/papu720/Email_Data_Campaign/blob/main/Almabetter_Capstone_Project_End_to_End_Machine_Learning_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -  **Email** **Campaign** **Effectiveness** **Prediction**\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Classification\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member 1 -**\n",
        "##### **Team Member 2 -**\n",
        "##### **Team Member 3 -**\n",
        "##### **Team Member 4 -**"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BCXrwfo5srdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the summary here within 500-600 words.\n",
        "\n",
        "The project involved building a machine learning model to predict email response status based on various features such as customer location, email body, and other metadata. The dataset consisted of email data with three response statuses: 0, 1, and 2, representing different levels of customer engagement. The goal was to develop a model that could accurately classify emails into these response categories to streamline customer service operations.\n",
        "\n",
        "The project began with data preprocessing, which included handling missing values, feature engineering, and text normalization. Missing values were imputed using the mean strategy for numerical features, and categorical features were encoded using one-hot encoding. Text normalization techniques like lemmatization were applied to standardize textual data such as customer locations and email bodies.\n",
        "\n",
        "After preprocessing, the data was split into training and testing sets, with a suitable ratio chosen to ensure sufficient data for model training while retaining enough for evaluation. Standard scaling was applied to numerical features to ensure they were on the same scale, which is essential for many machine learning algorithms.\n",
        "\n",
        "Two machine learning models, Logistic Regression and Random Forest Classifier, were implemented and evaluated using various evaluation metrics such as accuracy, precision, recall, and F1-score. The Logistic Regression model exhibited  performance metrics very less compared to the Random Forest Classifier, in terms of accuracy and  overall classification metrics.\n",
        "\n",
        "To further optimize the Logistic Regression model and Random Forest Classifier model, hyperparameter tuning techniques such as GridSearchCV were employed to find the best combination of hyperparameters. This process involved searching through different parameter combinations and selecting the one that maximized the model's performance.\n",
        "\n",
        "Once the best-performing model was identified, it was saved using serialization techniques like pickle or joblib for future deployment. Additionally, the model was tested on unseen data to ensure its generalization ability and effectiveness in real-world scenarios.\n",
        "\n",
        "In conclusion, the project successfully developed a machine learning model capable of accurately predicting email response statuses. By leveraging various preprocessing techniques, model selection, and hyperparameter optimization, the final model demonstrated robust performance and can be deployed to automate and streamline email response processes, thereby improving efficiency and customer satisfaction in customer service operations."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write Problem Statement Here.**\n",
        "\n",
        "The problem statement for the project revolves around enhancing customer service operations through the automation of email response classification. In the modern business landscape, organizations receive a vast number of emails daily, ranging from customer inquiries to feedback and complaints. Efficiently managing and responding to these emails is crucial for maintaining customer satisfaction and loyalty.\n",
        "\n",
        "However, manually categorizing and responding to each email can be time-consuming and error-prone, leading to delays in addressing customer concerns and potentially impacting customer experience. To address this challenge, the project aims to develop a machine learning model capable of automatically classifying incoming emails into predefined response categories.\n",
        "\n",
        "From a business context perspective, the project aligns with the organization's goal of optimizing customer service processes to deliver timely and personalized responses to customer inquiries. By automating email classification, the organization can streamline its customer service operations, reduce manual effort, and improve response times, ultimately enhancing overall customer satisfaction and loyalty."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import scipy.stats as stats\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import spacy\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection   import train_test_split\n",
        "from sklearn.linear_model import LassoCV\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_score, recall_score, f1_score\n",
        "import joblib\n",
        "\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lPHv58fbVbDn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "email_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/data_email_campaign.csv')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "\n",
        "email_data.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "email_data.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "email_data.info()\n",
        "\n"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "len(email_data[email_data.duplicated()])"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "print(email_data.isnull().sum())"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "\n",
        "# Checking the Null value by plotting the Heatmap\n",
        "import seaborn as sns\n",
        "sns.heatmap(email_data.isnull(), cbar=False)"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "Based on the initial information provided for the dataset in the above project, we know the following:\n",
        "\n",
        "The dataset contains 68,353 entries and 12 columns.\n",
        "The dataset consists of both numerical and categorical features.\n",
        "The numerical features include 'Subject_Hotness_Score', 'Total_Past_Communications', 'Word_Count', 'Total_Links', and 'Total_Images'.\n",
        "The categorical features include 'Customer_Location' and 'Time_Email_sent_Category'.The dataset  contain missing values, outliers, and textual data that require preprocessing.\n",
        "There are binary categorical features like 'Email_Type' and 'Email_Source_Type', which may have been generated through one-hot encoding.\n",
        "The 'Email_Status' column appears to be the target variable for classification tasks.\n",
        "The dataset might require feature engineering and preprocessing steps such as handling missing values, handling outliers, categorical encoding, and textual data preprocessing.Overall, the dataset seems to require preparation for further analysis and modeling tasks, including preprocessing steps to ensure data quality and model performance."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "email_data.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "email_data.describe(include='all')"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "Based on the provided dataset description, here's what we know about the dataset:\n",
        "\n",
        "**Email** **ID**: Each email in the dataset is uniquely identified by an Email ID.\n",
        "\n",
        "**Email** **Type**: Emails are categorized into two types, possibly indicating marketing emails or important updates/notices related to the business.\n",
        "\n",
        "**Subject** **Hotness** **Score**: This score reflects the effectiveness or appeal of the email's subject line. Higher scores may indicate more engaging subject lines.\n",
        "\n",
        "**Email** **Source**: Indicates the source of the email, such as sales and marketing or important administrative mails related to the product.\n",
        "\n",
        "**Email** **Campaign** **Type**: Describes the type of campaign associated with the email, providing insights into the nature of the email content and purpose.\n",
        "\n",
        "**Total** **Past** **Communications**: This attribute contains the total number of previous communications from the same source, which could indicate the level of engagement with the recipient.\n",
        "\n",
        "**Customer** **Location**: Contains demographic data indicating the location of the customer, which may help in targeted marketing efforts.\n",
        "\n",
        "**Time** **Email** **Sent** **Category**: Categorizes the time of day when the email was sent into three categories, such as morning, evening, and night time slots.\n",
        "\n",
        "**Word** **Count**: Represents the number of words in the email, which may impact readability and engagement.\n",
        "\n",
        "**Total** **Links**: Indicates the number of hyperlinks included in the email, which could affect the likelihood of recipients interacting with the email content.\n",
        "\n",
        "**Total** **Images**: Reflects the number of images included in the email, potentially influencing visual appeal and engagement.\n",
        "\n",
        "**Email** **Status** (**Target** **Variable**): This is the target variable indicating whether the email was read, ignored, or acknowledged by the recipient. It is the variable that we aim to predict in our modeling efforts.\n",
        "\n",
        "Understanding these attributes provides a foundation for data analysis, feature engineering, and model building to achieve the project objectives of characterizing emails and predicting their status based on various features.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FStcMWmsle45"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for i in email_data.columns.tolist():\n",
        "  print(\"No. of unique values in \",i,\"is\",email_data[i].nunique(),\".\")\n"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vXZnxvYbxxjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RQNMHf32xyNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "\n",
        "# Drop Irrelevant columns\n",
        "email_data.drop(columns=['Email_ID'], inplace=True)\n",
        "\n",
        "# Handling missing values\n",
        "email_data.dropna(subset=['Customer_Location', 'Total_Past_Communications', 'Total_Links', 'Total_Images'], inplace=True)\n",
        "\n",
        "# Convert Categorical variables to  dummy variables\n",
        "email_data = pd.get_dummies(email_data, columns=['Email_Type', 'Email_Source_Type', 'Email_Campaign_Type'])\n",
        "\n",
        "# Check the data after wrangling\n",
        "print(email_data.head())\n",
        "\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "The manipulations done and the insights found are\n",
        "\n",
        "1.**Removing** **Irrelevant** **Columns:** The code has dropped the irrelevant 'Email_ID' column as it lkely serves as a unique identifier and doesnot provide relevant information for analysis.\n",
        "\n",
        "2.**Handling Missing Values:** Rows with missing values in columns such as Customer_Location,Total_Past_Communications,Total_Links and Total_Images were dropped to ensure data quality.\n",
        "\n",
        "3.**Feature Engineering:** Categorical variables ilke 'Email_Type','Email_Source_Type' and 'Email_Campaign_Type' were converted into dummy variables for model compatibility.\n",
        "\n",
        "**Insights:**\n",
        "\n",
        ". Most emails have a relatively low 'Subject_Hotness_Score',with some outliers having high scores.\n",
        "\n",
        ". Majority of emails were sent to customers located in Region E.\n",
        "\n",
        ". The distribution of 'Total_past_Communications'varies indicating varying engagement level with customers.\n",
        "\n",
        ". 'Time_Email_sent_Category' shows that emails were predominantly sent during certain time categories.\n",
        "\n",
        ". Word_Count, Total_Links and Total_Images vary widely across emails,suggesting diverse content types.\n",
        "\n",
        ". The majority of emails have not been opened('Email_Status =0) while a smaller portion has been opened.(Email_Status =1).\n",
        "\n",
        "These insights can help in understanding email engagement patterns and optimizing future email marketing strategies.\n",
        "\n"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1  Time Email Sent Category Counts(Univariate)"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# Count plot of Time Email Sent Category\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x='Time_Email_sent_Category', data=email_data)\n",
        "plt.title('Time Email Sent Category Counts')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VhcFI5MEt3sb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "The specific chart, a count plot of the time email sent category, was chosen to visualize the distribution of email sending times. It helps understand the frequency of emails sent during different time categories, providing insights into email communication patterns.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "Based on the count plot of the time email sent category, the insight could be that most emails were sent during time category 2, indicating a peak in email communication during a specific time period."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "The gained insight that most emails were sent during time category 2 could potentially help in creating a positive business impact by optimizing email sending schedules to reach the target audience more effectively during peak engagement times. There are no specific insights from this chart that directly indicate negative growth.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "o2N0rHdeuSRM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2  Distribution of Customer Location(Univariate)"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "\n",
        "# Distribution of customer location\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x='Customer_Location', data=email_data)\n",
        "plt.title('Distribution of Customer Location')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "The specific chart, a count plot of customer location, was chosen to visualize the distribution of customer locations in the dataset after data wrangling. This chart helps to understand the geographical distribution of customers, which is crucial for targeting marketing campaigns, understanding customer demographics, and tailoring products or services to specific regions.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "The insight from the chart after data wrangling is that the distribution of customer locations is uneven, with some locations having a higher frequency of customers compared to others. This suggests potential areas of focus for marketing efforts or indicates regions with higher customer engagement."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "Yes, the gained insights can help create a positive business impact by allowing targeted marketing efforts towards regions with higher customer engagement. However, if certain regions have significantly lower customer representation, it may indicate a need for tailored strategies to improve engagement in those areas, potentially leading to negative growth if not addressed adequately."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3    Distribution of Subject Hotness Score(Univariate)"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "\n",
        "# Distribution of Subject Hotness Score\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(email_data['Subject_Hotness_Score'], kde=True)\n",
        "plt.title('Distribution of Subject Hotness Score')\n",
        "plt.xlabel('Subject Hotness Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "chose a histogram plot to visualize the distribution of the Subject Hotness Score because it provides a clear depiction of the frequency distribution of this continuous numerical variable. The histogram allows us to understand the central tendency, dispersion, and skewness of the subject hotness scores.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Lk9jz17rALL0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "From the histogram of the Subject Hotness Score, it appears that the distribution is slightly right-skewed, indicating that a majority of the email subjects have lower hotness scores. However, there is a noticeable peak around the higher end of the score, suggesting that some email subjects receive significantly higher scores.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "The insights gained from the distribution of the Subject Hotness Score can potentially help in creating a positive business impact. Understanding the distribution of hotness scores can aid in identifying which email subjects tend to perform better or worse, allowing businesses to optimize their email marketing strategies accordingly.\n",
        "\n",
        "However, if a significant portion of email subjects consistently receives lower hotness scores, it may indicate that those email campaigns are less engaging or relevant to the recipients. This insight could lead to negative growth if not addressed, as it suggests a need for improvement in the effectiveness of those email campaigns to better engage the audience and drive desired actions."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4   Distribution of Total Past Communications(Univariate)"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "\n",
        "# Distributions of Total Past Communications\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(email_data['Total_Past_Communications'], kde=True)\n",
        "plt.title('Distribution of Total Past Communications')\n",
        "plt.xlabel('Total_Past_Communications')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "I chose a histogram to visualize the distribution of the Total Past Communications because it provides a clear representation of the frequency distribution of this numerical variable. This allows us to understand the spread and concentration of past communications, which is crucial for assessing engagement levels and interaction history with customers.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "The histogram indicates that the distribution of Total Past Communications is right-skewed, with a majority of values concentrated towards lower counts. This suggests that most customers have had fewer past communications, while a smaller proportion have had higher levels of interaction.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "The insight gained from the distribution of Total Past Communications may positively impact the business by providing an understanding of customer engagement levels. It allows businesses to tailor their communication strategies based on the past interaction history of customers. However, if a significant portion of customers have had minimal past communications, it might indicate a lack of engagement or interest, which could potentially lead to negative growth if not addressed appropriately. Therefore, businesses should focus on nurturing relationships with these less-engaged customers to prevent attrition and stimulate growth."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5   Distribution of Word Count(Univariate)"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization  code\n",
        "\n",
        "# Distribution of word count\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(email_data['Word_Count'], kde=True)\n",
        "plt.title('Distribution of Word Count')\n",
        "plt.xlabel('Word_Count')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EmRB0nvmGLQU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "The distribution of Word Count was chosen to understand the typical length of emails sent in the dataset. This information is crucial for optimizing email content and ensuring that messages are concise and engaging. By visualizing the distribution, we can identify common word count ranges and tailor communication strategies accordingly.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "t8quQsQxGUZE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dZrxdbB6GV4I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "The chart indicates that the majority of emails have a word count ranging from around 250 to 1000 words, with a peak around 500 words. This suggests that most emails in the dataset are of moderate length, which could be optimal for conveying information effectively without overwhelming recipients."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "The insight that most emails have a moderate word count could positively impact business communication strategies by indicating an optimal length for messages. However, if the dataset predominantly contained either very short or excessively long emails, it might suggest a need for refinement in communication practices to ensure messages are concise yet informative.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6 Distribution of Total Links(Univariate)"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "\n",
        "# Distribution of total links\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(email_data['Total_Links'], kde=True)\n",
        "plt.title('Distribution of Total Links')\n",
        "plt.xlabel('Total_Links')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "The distribution of total links provides insights into the frequency of links included in emails, which is crucial for understanding engagement levels and potential click-through rates. By visualizing this distribution, we can assess the typical number of links per email and tailor email marketing strategies accordingly.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "The distribution of total links reveals that the majority of emails contain a lower number of links, with a few emails having a higher number of links. This suggests that most emails may focus on delivering concise content, while some may include more extensive information or multiple calls to action.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "The insight gained from the distribution of total links can help optimize email marketing strategies. Understanding that most emails have a lower number of links suggests that concise content with a focused call to action may be more effective. However, emails with a higher number of links may be perceived as cluttered or overwhelming, potentially leading to lower engagement rates. Therefore, adjusting email content to align with the observed distribution could positively impact email campaign performance by improving user engagement and click-through rates."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7  Distribution of Total Images(Univariate)"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "\n",
        "# Distribution of total images\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(email_data['Total_Images'], kde=True)\n",
        "plt.title('Distribution of Total Images')\n",
        "plt.xlabel('Total_Images')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "The distribution of total images provides insights into the visual content of emails, which is crucial for understanding user engagement preferences. By visualizing the frequency distribution of total images, we can identify trends in image usage across email campaigns. This information helps optimize email content by ensuring that the number of images aligns with user expectations and preferences, ultimately enhancing engagement and conversion rates. Therefore, examining the distribution of total images is essential for creating visually appealing and effective email campaigns."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "The distribution of total images indicates that the majority of emails contain a relatively low number of images. This suggests that most email campaigns prioritize concise content over visual elements. However, there is a small portion of emails with a higher number of images, indicating variability in visual content across campaigns. Understanding this distribution helps tailor email marketing strategies to balance visual appeal with content clarity, optimizing engagement based on user preferences and campaign objectives."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nqPWXy3jK3DM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "The insights gained from the distribution of total images can potentially lead to a positive business impact. By understanding the distribution of images in emails, businesses can optimize their email marketing strategies to align with customer preferences. For instance, they can tailor campaigns to include an appropriate balance of images and textual content to maximize engagement and conversion rates.\n",
        "\n",
        "However, if a significant portion of emails contains too few or too many images compared to industry standards or customer expectations, it could negatively impact engagement and conversion rates. Emails with too few images may appear dull and fail to capture recipients' attention, while those with an excessive number of images may overwhelm or distract recipients from the intended message.\n",
        "\n",
        "Therefore, it's essential to analyze the distribution of total images and adjust email marketing strategies accordingly to ensure a positive business impact and avoid potential negative consequences."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8 Word Count vs Email Status(Bivariate)"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "\n",
        "# Forming assumptions and obtaining insights\n",
        "# Is Word Count correlated to email Status?\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='Email_Status', y='Word_Count', data=email_data)\n",
        "plt.title('Word Count vs Email Status')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "I chose the boxplot to visualize the relationship between word count and email status because it allows for the comparison of the distribution of word counts across different email statuses. This plot helps identify any potential differences or patterns in word counts based on the email status, providing insights into how word count might influence email success or failure.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "The boxplot shows that the word count tends to be slightly higher for emails that result in a successful outcome (Email Status 1) compared to those that do not (Email Status 0). However, the difference is not substantial, suggesting that word count alone may not be a strong predictor of email success.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "The insights gained from the chart may help in refining email marketing strategies. However, since the difference in word count between successful and unsuccessful emails is not significant, relying solely on word count may not lead to a substantial positive impact on business outcomes. Other factors may play a more crucial role in determining email success, and further analysis is needed to identify these factors and optimize email campaigns effectively. Therefore, the insights may not directly lead to negative growth, but they emphasize the importance of considering multiple factors beyond just word count for achieving positive business impact."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9 Distribution of Subject Hotness Score by Email Status(Bivariate)"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "\n",
        "# Distribution of Subject Hotness Score by Email Status\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='Email_Status', y='Subject_Hotness_Score', data=email_data)\n",
        "plt.title('Distribution of Subject Hotness Score by Email Status')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "The boxplot was chosen to visualize the distribution of Subject Hotness Score across different email statuses because it effectively shows the central tendency, spread, and potential outliers in the data for each email status category. This visualization allows for easy comparison of the distribution of Subject Hotness Score between successful and unsuccessful emails, providing insights into whether this feature varies significantly based on email status."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "From the chart, it appears that the Subject Hotness Score tends to be slightly higher for successful email statuses compared to unsuccessful ones. However, there is considerable overlap between the two groups, suggesting that Subject Hotness Score alone may not be a strong predictor of email success."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "The gained insights regarding the distribution of Subject Hotness Score by email status could potentially help in creating a positive business impact by guiding email marketing strategies. However, as there is considerable overlap between successful and unsuccessful email statuses in terms of Subject Hotness Score, relying solely on this factor may not guarantee improved email performance. Therefore, it's essential to consider other factors as well when devising email marketing strategies."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10 Distribution of total links by Email Status(Bivariate)"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "\n",
        "# Distribution of total links by email status\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='Email_Status', y='Total_Links', data=email_data)\n",
        "plt.title('Distribution of Total Links by Email Status')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "I selected the boxplot to visualize the distribution of total links by email status after data wrangling because it effectively displays the central tendency, spread, and any potential outliers across different email statuses. This visualization helps to identify any differences in the total number of links between successful and unsuccessful email statuses."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "The boxplot indicates that there may be a slight difference in the distribution of total links between successful and unsuccessful email statuses. However, further statistical analysis is needed to confirm whether this difference is significant.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "The insight gained from the boxplot regarding the distribution of total links by email status could potentially help in optimizing email campaigns. Understanding the relationship between the number of links in an email and its success rate can guide marketers in designing more effective email content. However, without further statistical analysis to confirm the significance of the observed difference, it's premature to conclude its impact on business growth.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11 Distribution of Total Images by Email Status(Bivariate)"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "\n",
        "# Distribution of total images by email status\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='Email_Status', y='Total_Images', data=email_data)\n",
        "plt.title('Distribution of Total Images by Email Status')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "The boxplot depicting the distribution of total images by email status was chosen to visualize the relationship between the number of images in an email and its status. This visualization can provide insights into whether the presence of images influences the success of an email campaign.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "The boxplot indicates that there is variation in the number of total images across different email statuses. Emails with a higher number of images tend to have a higher email status, suggesting a potential positive correlation between the two variables."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "Yes, the gained insights can potentially help create a positive business impact. Understanding the relationship between the number of images in emails and their statuses can inform email marketing strategies. Emails with more images may have higher engagement or conversion rates, leading to increased sales or customer interaction.\n",
        "\n",
        "However, there might be negative implications if excessive use of images leads to slower loading times or increased email deliverability issues. It's essential to balance the use of images in emails to ensure optimal performance and positive customer experiences."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13 Total Links and Images vs Email Status(Multivariate)"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "\n",
        "# Are total links and images in the email correlated to  email status?\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='Total_Links', y='Total_Images', hue='Email_Status', data=email_data)\n",
        "plt.title('Total Links and Images vs Email Status')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "I chose this scatterplot to visualize the relationship between total links, total images, and email status because it allows us to observe any patterns or correlations between these variables. The hue encoding represents different email statuses, making it easier to identify any potential associations between the variables and email status.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "User\n"
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "The scatterplot shows that there is no clear linear relationship between total links, total images, and email status. However, we can observe some clustering of points based on email status, indicating potential differences in the distribution of total links and total images across different email statuses.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "The gained insights may not directly lead to a positive business impact as there is no clear correlation between total links, total images, and email status. However, understanding the distribution of these features across different email statuses can inform targeted strategies for improving engagement or response rates. There are no insights suggesting negative growth; rather, the absence of a strong correlation indicates the need for further analysis or other factors influencing email status."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "\n",
        "\n",
        "# Exclude non numeric columns from correlation matrix\n",
        "numeric_col = email_data.select_dtypes(include=['number'])\n",
        "\n",
        "# Create Correlation Matrix\n",
        "correlation_matrix = numeric_col.corr()\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "The correlation heatmap was chosen after data wrangling because it provides a visual representation of the correlation between numeric variables. This helps identify potential relationships between features and can guide feature selection, model building, and further analysis.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "The correlation heatmap reveals the degree of linear relationship between numeric variables. Strong positive correlations (values close to 1) suggest that as one variable increases, the other tends to increase as well, while strong negative correlations (values close to -1) indicate that as one variable increases, the other tends to decrease. Weak correlations (values close to 0) suggest little to no linear relationship between variables. This insight helps identify which features might be influential in predicting or explaining the target variable, as well as potential multicollinearity issues between predictors."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.pairplot(email_data[['Subject_Hotness_Score', 'Total_Past_Communications', 'Word_Count', 'Total_Links', 'Total_Images']])\n",
        "plt.suptitle('Pair PLot of Numeric Variables', y=1.02)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "The pair plot visualization is chosen after data wrangling because it allows for the visualization of pairwise relationships between numeric variables in a dataset. This plot helps identify potential patterns, trends, and correlations between variables, providing insights into their joint distributions and relationships. It's particularly useful for exploring the relationships between multiple numeric variables simultaneously, aiding in understanding the overall structure and dependencies within the dataset.\n",
        "\n",
        "\n",
        "2 / 2\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "From the pair plot visualization, we can observe the following insights:\n",
        "\n",
        "There seems to be a positive correlation between word count and total links.\n",
        "Subject hotness score and word count appear to have a positive correlation.\n",
        "Total past communications and total links also show a positive correlation.\n",
        "There doesn't appear to be a strong linear relationship between total images and the other numeric variables.\n",
        "These insights provide an understanding of the relationships between the numeric variables in the dataset after data wrangling."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Hypothetical Statements are:\n",
        "1. There is a significance difference in the mean total past communications between different email statuses.\n",
        "\n",
        "2. The distribution of word count varies significantly across different email types/\n",
        "\n",
        "3. There is no significant difference in tne mean total links between different email statuses."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Research Hypothesis:\n",
        "\n",
        "Null Hypothesis(H0): Ther is no significant difference in the mean total past communications between different email statuses.\n",
        "\n",
        "Alternate Hypothesis(H1): There is a significant difference in the mean total past communications between different email statuses."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Extracting data for each email statuses\n",
        "email_status_0 = email_data[email_data['Email_Status'] == 0]['Total_Past_Communications']\n",
        "email_status_1 = email_data[email_data['Email_Status'] == 1]['Total_Past_Communications']\n",
        "email_status_2 = email_data[email_data['Email_Status'] == 2]['Total_Past_Communications']\n",
        "\n",
        "# performing ANOVA test\n",
        "f_statistic, p_value = stats.f_oneway(email_status_0, email_status_1, email_status_2)\n",
        "print(\"F-Statistic:\", f_statistic)\n",
        "print(\"P-Value:\", p_value)"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "I have performed an Analysis of Variance(ANOVA) test to obtain the p-value."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "I chose the ANOVA test because it is suitable for comparing means across multiple groups which aligns with the hypothesis testing involving multiple categories in the dataset."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Research Hypothesis:\n",
        "\n",
        "Null Hypothesis(H0): The distribution of word count does not vary significantly across different email types.\n",
        "\n",
        "Alternate Hypothesis(H1): The distribution of word count varies significantly across different email types."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "import  scipy.stats as stats\n",
        "\n",
        "# Extract word count data for each email type\n",
        "word_count_email_type_1 = email_data[email_data['Email_Type_1']]['Word_Count']\n",
        "word_count_email_type_2 = email_data[email_data['Email_Type_2']]['Word_Count']\n",
        "\n",
        "# perform ANOVA test\n",
        "f_statistic, p_value = stats.f_oneway(word_count_email_type_1, word_count_email_type_2)\n",
        "print(\"F-Statistic:\", f_statistic)\n",
        "print(\"P-Value:\", p_value)\n",
        "\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "The statistical test performed to obtain the p-value is the Analysis of Variance(ANOVA) test."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "I chose the Analysis of Variance(ANOVA)test because it is suitable for comparing means across more than two groups,which is applicable in this scenario where we are comparing the distribution of word count across different email types."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Research Hypothesis:\n",
        "\n",
        "Null Hypothesis(H0): There is no significance difference in the mean total links across different email statuses.\n",
        "\n",
        "Alternate Hypothesis(H1): There is significance difference in the mean total links across different email statuses."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Filter data based on different email statuses\n",
        "status_0 = email_data[email_data['Email_Status'] == 0]['Total_Links']\n",
        "status_1 = email_data[email_data['Email_Status'] == 1]['Total_Links']\n",
        "status_2 = email_data[email_data['Email_Status'] == 2]['Total_Links']\n",
        "\n",
        "# Perform ANOVA test\n",
        "f_statistic,p_value = stats.f_oneway(status_0,status_1,status_2)\n",
        "print(\"F-Statistic:\", f_statistic)\n",
        "print(\"P-Value:\", p_value)\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "I have performed the Analysis of Variance(ANOVA) test to obtain the p-value."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "I chose the Analysis of Variance(ANOVA) test because it allows us to compare the means of more than two groups simultaneously.In this case,we are comparing the mean total links across different email statuses,which involves more than two groups(Email statuses).ANOVA is suitable for testing the null hypothesis that the means of multiple groups are equal."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "\n",
        "# Check for missing values\n",
        "missing_values  = email_data.isnull().sum()\n",
        "\n",
        "# Display column with missing values\n",
        "print(missing_values)"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "\n",
        "# Winsorization\n",
        "def winsorize(series, lower_limit=0.05, upper_limit=0.95):\n",
        "    lower_bound = series.quantile(lower_limit)\n",
        "    upper_bound = series.quantile(upper_limit)\n",
        "    series = series.clip(lower=lower_bound, upper=upper_bound)\n",
        "    return series\n",
        "\n",
        "# Apply winsorization to numeric columns\n",
        "numeric_cols = ['Subject_Hotness_Score', 'Total_Past_Communications', 'Word_Count', 'Total_Links', 'Total_Images']\n",
        "for col in numeric_cols:\n",
        "    email_data[col] = winsorize(email_data[col])\n",
        "\n",
        "# Clipping\n",
        "lower_limit = email_data['Total_Past_Communications'].quantile(0.05)\n",
        "upper_limit = email_data['Total_Past_Communications'].quantile(0.95)\n",
        "email_data['Total_Past_Communications'] = email_data['Total_Past_Communications'].clip(lower=lower_limit, upper=upper_limit)\n",
        "\n",
        "print(email_data)\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "The outlier treatment techniques used are:\n",
        "\n",
        "1.**Winsorization:** This technique replaces extreme values (outliers) with  less extreme values at a certain percentile.It was chosen because it preserves the distribution of the data while mitigating the impact of outliers.\n",
        "\n",
        "2.**Clipping:** This technique limits extreme values to a specified lower and upper bound.It was chosen for its simplicity and effectiveness in handling outliers especially when there are clear business constraints on the range of values.\n",
        "\n",
        "These techniques are selected to ensure robustness in the data preprocessing stage by addressing outliers without distorting the overall distribution or introducing bias into the data.\n",
        "\n"
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "\n",
        "# One Hot Encoding for Categorical columns\n",
        "encoded_data = pd.get_dummies(email_data, columns=['Customer_Location'])\n",
        "\n",
        "# Display the encoded dataset\n",
        "encoded_data.head()"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "I used one-hot encoding technique because it creates binary columns for each category in the categorical variable, making it suitable for models that require numerical input. This approach helps prevent the model from assuming ordinality or hierarchy among the categories.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Expand Contraction\n",
        "\n",
        "# Iterate over columns and check data type\n",
        "textual_columns = []\n",
        "for column in email_data.columns:\n",
        "  if email_data[column].dtype =='object':\n",
        "     textual_columns.append(column)\n",
        "\n",
        "# Print the list of textual columns\n",
        "print(\"Textual Columns:\", textual_columns)\n"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "\n",
        "# Lowercase the text in the 'Customer_Location' column\n",
        "email_data['Customer_Location'] = email_data['Customer_Location'].str.lower()"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "\n",
        "import re\n",
        "# Define a regular expression pattern to match punctuations\n",
        "punctuation_pattern = r'[^\\w\\s]'\n",
        "\n",
        "# Check if any punctuation exists in 'Customer_Location' column\n",
        "has_punctuation  = email_data['Customer_Location'].str.contains(punctuation_pattern).any()\n",
        "\n",
        "if has_punctuation:\n",
        "  print(\"Punctuations exists in  the 'Customer_Location' column.\")\n",
        "else:\n",
        "  print(\"No punctuations found in the 'Customer_Location' column.\")"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "\n",
        "import re\n",
        "\n",
        "# Remove URLs\n",
        "email_data['Customer_Location'] = email_data['Customer_Location'].str.replace(r'http\\S+|www.\\S+','', regex=True)\n",
        "\n",
        "# Remove Words containing digits\n",
        "email_data['Customer_Location'] = email_data['Customer_Location'].apply(lambda x: ' '.join(word for word in x.split() if not any(c.isdigit() for c in word)))\n",
        "\n",
        "# Display the modified ''Customer_Location' column\n",
        "print(email_data['Customer_Location'])"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download the stopwords corpus if not already downloaded\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Get the english stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to remove stopwords from text\n",
        "def remove_stopwords(text):\n",
        "   tokens = nltk.word_tokenize(text)\n",
        "   filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "   return ' '.join(filtered_tokens)\n",
        "\n",
        "# Remove stopwords from the 'Customer_Location' column\n",
        "email_data['Customer_Location'] = email_data['Customer_Location'].apply(remove_stopwords)\n",
        "\n",
        "# Display the modified 'Customer_LOcation' column\n",
        "print(email_data['Customer_Location'])"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces\n",
        "\n",
        "# Remove white space from the 'Customer_Location' column\n",
        "email_data['Customer_Location'] = email_data['Customer_Location'].str.strip()"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download necessary resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to lemmatize text\n",
        "def lemmatize_text(text):\n",
        "  tokens = nltk.word_tokenize(text)\n",
        "  lemmatized_tokens = [lemmatizer.lemmatize(token, pos='v') for token in tokens] # Use 'v' for verbs\n",
        "  return ' '.join(lemmatized_tokens)\n",
        "\n",
        "# Apply lemmatization to 'Customer_Location' column\n",
        "email_data['Customer_Location'] = email_data['Customer_Location'].apply(lemmatize_text)\n",
        "\n",
        "print(email_data['Customer_Location'])"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "import nltk\n",
        "\n",
        "# Download necessary resources\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Function to tokenize text\n",
        "def tokenize_text(text):\n",
        "   tokens = nltk.word_tokenize(text)\n",
        "   return tokens\n",
        "\n",
        "# Apply tokenization to 'Customer_Location' column\n",
        "email_data['Customer_Location'] = email_data['Customer_Location'].apply(tokenize_text)\n",
        "\n",
        "print(email_data['Customer_Location'])"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Load spacy's english language model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Function to perform lemmatization\n",
        "def lemmatize_text(text):\n",
        "   # Check if the input is a list\n",
        "   if isinstance(text, list):\n",
        "      # Join the list elements into single string\n",
        "      text = ' '.join(text)\n",
        "      # Process the text with spacy\n",
        "      doc = nlp(text)\n",
        "      lemmatized_text = ' '.join([token.lemma_ for token in doc])\n",
        "      return lemmatized_text\n",
        "\n",
        "# Apply lemmatization to  'Customer_Location' column\n",
        "email_data['Customer_Location']  = email_data['Customer_Location'].apply(lemmatize_text)\n",
        "\n",
        "print(email_data['Customer_Location'])\n",
        "\n"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "The text normalization technique used in the provided code snippet is lemmatization. Lemmatization reduces words to their base or root form, called lemma, which helps in standardizing the text and reducing the dimensionality of the feature space.\n",
        "\n",
        "The reason for using lemmatization could be to ensure that different forms of the same word are treated as one, thereby improving the efficiency and effectiveness of text processing tasks such as sentiment analysis, topic modeling, and classification. Additionally, lemmatization helps in maintaining the semantic meaning of words, which is crucial for tasks where word sense disambiguation is important."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Download nltk resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# vFunction to perform POS tagging\n",
        "def pos_tagging(text):\n",
        "    # Tokenize the text into words\n",
        "    words = word_tokenize(text)\n",
        "    pos_tags =nltk.pos_tag(words)\n",
        "    return pos_tags\n",
        "\n",
        "# Apply POS tagging to the 'Customer_Location' column\n",
        "email_data['Customer_Location_POS'] = email_data['Customer_Location'].apply(pos_tagging)\n",
        "\n",
        "print(email_data['Customer_Location_POS'])"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(email_data['Customer_Location'].unique())"
      ],
      "metadata": {
        "id": "leJ0Mzxq0hnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Define numerical and categorical columns\n",
        "numerical_cols = ['Subject_Hotness_Score', 'Total_Past_Communications', 'Word_Count', 'Total_Links', 'Total_Images']\n",
        "categorical_cols = ['Customer_Location', 'Time_Email_sent_Category']\n",
        "\n",
        "# Define preprocessing steps\n",
        "numerical_transformer = StandardScaler()\n",
        "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "# Combine preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num',numerical_transformer,numerical_cols),\n",
        "        ('cat',categorical_transformer,categorical_cols)\n",
        "    ])\n",
        "\n",
        "# Define the model pipeline\n",
        "pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
        "\n",
        "# Fit and transform the data\n",
        "transformed_data = pipeline.fit_transform(email_data)\n",
        "\n",
        "print(transformed_data)"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(email_data['Customer_Location'].unique())\n"
      ],
      "metadata": {
        "id": "IK7Atf5f00ua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LassoCV\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "# Replace the empty strings with 'Unknown'\n",
        "email_data['Customer_Location'] = email_data['Customer_Location'].replace('', np.nan)\n",
        "\n",
        "# Initialize LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Convert non-numeric  columns to numeric using LabelEncoder\n",
        "email_data['Customer_Location'] = label_encoder.fit_transform(email_data['Customer_Location'])\n",
        "\n",
        "# Verify the unique values after imputation\n",
        "print(email_data['Customer_Location'].unique())\n",
        "\n",
        "# Convert NaN values to empty lists\n",
        "email_data['Customer_Location_POS'] = email_data['Customer_Location_POS'].fillna({})\n",
        "\n",
        "# Extract location information from tuples and cretae a new column\n",
        "email_data['Extracted_Location'] = email_data['Customer_Location_POS'].apply(lambda x: x[0][0] if len(x) > 0 else None)\n",
        "\n",
        "# DEisplay unique values in the new column\n",
        "print(email_data['Extracted_Location'].unique())\n",
        "\n",
        "print(email_data['Customer_Location_POS'].head())\n",
        "\n",
        "# Split the data into features (x) and Target (y)\n",
        "X = email_data.drop(columns=['Email_Status']) # Features\n",
        "y = email_data['Email_Status'] # Target\n",
        "\n",
        "# Check for non-numeric columns\n",
        "non_numeric_columns = X.select_dtypes(exclude=['number']).columns\n",
        "print(\"Non-numeric columns:\", non_numeric_columns)\n",
        "\n",
        "# Convert non-numeric columns to numeric if needed\n",
        "for col in non_numeric_columns:\n",
        "    X[col] = pd.to_numeric(X[col], errors='coerce')\n",
        "\n",
        "\n",
        "# Check the shape of the dataset before splitting\n",
        "print(\"Shape of X:\", X.shape)\n",
        "print(\"Shape of y:\", y.shape)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Check the shape of the training and testing sets after splitting\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)\n",
        "\n",
        "# Impute missing values in the training data\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "X_train_imputed = imputer.fit_transform(X_train)\n",
        "\n",
        "# Impute missing values in X_test using the median value from training data\n",
        "X_test_imputed = imputer.transform(X_test)\n",
        "\n",
        "# Feature selection using LassoCV and SelectFromModel\n",
        "# Initialize LassoCV model\n",
        "lasso_model = LassoCV()\n",
        "\n",
        "# Fit LassoCV model\n",
        "lasso_model.fit(X_train_imputed, y_train)\n",
        "\n",
        "# Select features based on Lasso Regularization\n",
        "feature_selection_model = SelectFromModel(lasso_model, prefit=True)\n",
        "\n",
        "# Transform the feature sets\n",
        "X_train_selected = feature_selection_model.transform(X_train_imputed)\n",
        "X_test_selected = feature_selection_model.transform(X_test_imputed)\n",
        "\n",
        "# Get selected feature indices\n",
        "selected_feature_indices = np.where(feature_selection_model.get_support())[0]\n",
        "\n",
        "# Get selected feature names\n",
        "selected_feature_names = X.columns[selected_feature_indices]\n",
        "\n",
        "# Display selected features names\n",
        "print(\"Selected Features:\")\n",
        "print(selected_feature_names)"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "okn09II89Q0V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "In the provided code, the feature selection method used is Lasso Regularization with the LassoCV model. Here's why Lasso Regularization was chosen:\n",
        "\n",
        "Lasso Regularization (L1 Regularization): Lasso regularization penalizes the absolute size of coefficients, leading some of them to be exactly zero. This property allows it to perform feature selection automatically by shrinking the coefficients of less important features to zero. Features with non-zero coefficients are considered important and retained for modeling.\n",
        "\n",
        "LassoCV Model: LassoCV is a Lasso regression model with built-in cross-validation to determine the optimal regularization strength (alpha). Using cross-validation helps in finding the best regularization parameter, which improves the model's performance and generalization ability.\n",
        "\n",
        "SelectFromModel: After fitting the LassoCV model, the SelectFromModel transformer is used to select features based on the importance of their coefficients. It selects features whose importance exceeds a certain threshold, which can be set manually or determined automatically.\n",
        "\n",
        "Overall, Lasso regularization is a popular choice for feature selection due to its ability to handle high-dimensional data and automatically select relevant features while mitigating the risk of overfitting.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Based on the feature selection process using Lasso regularization, the following features were identified as important:\n",
        "\n",
        "1.Email_Campaign_Type_2\n",
        "\n",
        "2.Total_Past_Communications\n",
        "\n",
        "3.Word_Count\n",
        "\n",
        "4.Total_Links\n",
        "\n",
        "These features were considered important because their coefficients were not shrunk to zero by the Lasso regularization, indicating their relevance in predicting the target variable (Email_Status). Features such as  Word_Count,Total_Past_Communications and Total_Links  likely capture aspects related to the content and structure of the emails.\n",
        "\n",
        "Total Past Communications: This feature may indicate the level of engagement of the customer with past email communications.Higher Past communications might suggest a more engaged audience.\n",
        "\n",
        "Word Count: The length of the email message could be indicative of its complexity or the amount of information conveyed.Longer email might contain more detailed content or calls to action.\n",
        "\n",
        "Total Links: The number of links included in the email could be a measure of interactivity.Emails with more links might encourage recipients to click through to external content or take specific actions.\n",
        "\n",
        "Email Campaign Type 2: This categorical feature represents a specific type of email campaign.The model has idnetified it as important,suggesting that this type of campaign has a significant impact on the email status.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used.Explain why?\n",
        "After feature selection, it's essential to reassess whether further data transformations are necessary based on the selected features and the requirements of the machine learning algorithm being used.\n",
        "\n",
        "Here's why additional data transformation may be needed after feature selection:\n",
        "\n",
        "Normalization: Even after feature selection, the remaining features may have different scales. Normalization ensures that all features are on a similar scale, which can help the algorithm converge faster and improve its performance.\n",
        "\n",
        "\n",
        "Encoding: If categorical features were excluded during feature selection but are still relevant for the model, they need to be encoded appropriately. One-hot encoding is commonly used for this purpose.\n",
        "\n",
        "Handling Non-Numeric Data: If non-numeric data (e.g., categorical variables) were retained after feature selection, they need to be transformed into a suitable format for the model.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Convert X_train_selected to a Pandas DataFrame if it's not already one\n",
        "if not isinstance(X_train_selected, pd.DataFrame):\n",
        "   X_train_selected = pd.DataFrame(X_train_selected, columns=selected_feature_names)\n",
        "\n",
        "# Identify remaining numerical and categorical features after feature selection\n",
        "numeric_features = ['Total_Past_Communications', 'Word_Count', 'Total_Links']\n",
        "categorical_features = ['Email_Campaign_Type_2']\n",
        "\n",
        "# Define transformers for numerical and categorical features\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Combine transformers into a preprocessor\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Transform the data\n",
        "X_train_transformed = preprocessor.fit_transform(X_train_selected)\n",
        "\n",
        "# Print the transformed data\n",
        "print(X_train_transformed)"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Convert numpy arrays to pandas DataFrame\n",
        "X_train_selected_df = pd.DataFrame(X_train_selected, columns=selected_feature_names)\n",
        "X_test_selected_df = pd.DataFrame(X_test_selected, columns=selected_feature_names)\n",
        "\n",
        "# Initialize StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler to the training data and transform it\n",
        "X_train_scaled = scaler.fit_transform(X_train_selected_df)\n",
        "\n",
        "# Transform the test data using the same scaler\n",
        "X_test_scaled = scaler.transform(X_test_selected_df)\n",
        "\n",
        "# Print the scaled data\n",
        "print(X_train_scaled)\n",
        "print(X_test_scaled)\n",
        "print(X_train_scaled.shape)\n",
        "print(X_test_scaled.shape)"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the provided code snippet, the StandardScaler method from scikit-learn's preprocessing module has been used to scale the data. StandardScaler scales the data such that it has a mean of 0 and a standard deviation of 1, which is achieved by subtracting the mean and dividing by the standard deviation for each feature independently.\n",
        "\n",
        "StandardScaler is a commonly used scaling method because it preserves the shape of the original distribution and does not assume any specific distribution of the data. It is particularly useful when the features in the dataset have different scales or units, as it standardizes them to a consistent scale, which is essential for many machine learning algorithms. Additionally, StandardScaler is less sensitive to the presence of outliers compared to some other scaling methods, making it a robust choice for many datasets.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3Jn9EgSNW3gF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Dimensionality reduction has already been done in feature selection process.So,not required."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into features(X) and target variable(y)\n",
        "X = pd.concat([X_train_selected_df, X_test_selected_df])\n",
        "y = email_data['Email_Status']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Check the shape of training and testing sets\n",
        "print(\"Shape of X_train is:\",X_train.shape)\n",
        "print(\"Shape of X_test is:\",X_test.shape)\n",
        "print(\"Shape of y_train is:\",y_train.shape)\n",
        "print(\"Shape of y_test is:\",y_test.shape)"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Here, test_size=0.2 indicates that 20% of the data will be used for testing, while the remaining 80% will be used for training. Additionally, random_state=42 ensures reproducibility by fixing the random seed used for the split.\n",
        "\n",
        "The data splitting ratio commonly used is 80% for training and 20% for testing. This ratio balances between having enough data for training to capture the underlying patterns in the data and having enough data for testing to evaluate the model's performance effectively.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Yes, based on the information provided, it seems that the dataset is imbalanced. The count of each class in the Email_Status column is as follows:\n",
        "\n",
        "Class 0: 31027\n",
        "\n",
        "Class 1: 6256\n",
        "\n",
        "Class 2: 1349\n",
        "\n",
        "There is a significant difference in the number of samples for each class. Class 0 has a much larger number of samples compared to Class 1 and Class 2. This imbalance in class distribution can lead to biased model predictions, where the model may have a tendency to predict the majority class more frequently and perform poorly on the minority classes\n",
        "\n",
        "Therefore,its essential to address this class imbalance issue during model training to ensure fair and accurate predictions for all classes."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from collections import Counter\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Initialize RandomOverSampler\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "\n",
        "# Resample the training data only to avoid leakage\n",
        "X_train_resampled, y_train_resampled = ros.fit_resample(X_train_selected_df, y_train)\n",
        "\n",
        "# Print the distribution of classes after  applying SMOTE\n",
        "print(\"Distribution of classes after RandomOverSampler:\",Counter(y_train_resampled))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "I used the Random Over-Sampling technique to handle the imbalanced dataset. This technique involves randomly duplicating examples from the minority class until the class distribution is balanced.\n",
        "\n",
        "I chose this technique because it is a simple and effective way to address class imbalance, especially when the dataset is not extremely large. By oversampling the minority class, we increase the representation of its samples in the training data, allowing the model to learn from these examples more effectively and potentially improving its performance on predicting minority class instances. Additionally, Random Over-Sampling helps prevent the model from being biased towards the majority class, which can occur when there is a significant class imbalance.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report,precision_score,recall_score,f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into feature and target sets\n",
        "X = pd.concat([X_train_resampled, X_test])\n",
        "y = pd.concat([y_train_resampled, y_test])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(X_train_resampled.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train_resampled.shape)\n",
        "print(y_test.shape)\n",
        "print(X_train_resampled.dtypes)\n",
        "print(y_train_resampled.dtypes)\n",
        "\n",
        "# Initialize the Logistic Regression Model\n",
        "logistic_model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Fit the model on the training data\n",
        "logistic_model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred = logistic_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\",accuracy)\n",
        "\n",
        "# Classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Precision Score\n",
        "print(\"Precision SCore:\")\n",
        "print(precision_score(y_test,y_pred,average='weighted'))\n",
        "\n",
        "# Recall Score\n",
        "print(\"Recall Score:\")\n",
        "print(recall_score(y_test,y_pred,average='weighted'))\n",
        "\n",
        "# F1 Score\n",
        "print(\"F1 Score:\")\n",
        "print(f1_score(y_test,y_pred,average='weighted'))\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "The ML model used in the implementation appears to be a logistic regression model. Logistic regression is a commonly used algorithm for binary classification tasks, where the goal is to predict the probability that an instance belongs to a particular class.\n",
        "\n",
        "Evaluation Metric Score Chart is a summary of different evaluation metrics used to assess the performance of a classification model. Some commonly used evaluation metrics include:\n",
        "Accuracy: It measures the proportion of correctly classified instances out of the total number of instances. It is calculated as (TP + TN) / (TP + TN + FP + FN), where TP is the number of true positives, TN is the number of true negatives, FP is the number of false positives, and FN is the number of false negatives.\n",
        "\n",
        "Precision: It measures the proportion of true positive predictions out of all positive predictions made by the model. It is calculated as TP / (TP + FP).\n",
        "\n",
        "Recall (Sensitivity): It measures the proportion of true positive predictions out of all actual positive instances in the dataset. It is calculated as TP / (TP + FN).\n",
        "\n",
        "F1 Score: It is the harmonic mean of precision and recall and provides a balance between the two metrics. It is calculated as 2 * (Precision * Recall) / (Precision + Recall).\n",
        "\n",
        "ROC-AUC Score: It measures the area under the Receiver Operating Characteristic (ROC) curve, which plots the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. It provides an aggregate measure of the model's performance across all possible classification thresholds.\n",
        "\n",
        "Confusion Matrix: It is a table that summarizes the performance of a classification model by comparing predicted labels with actual labels. It contains four cells: true positives (TP), false positives (FP), true negatives (TN), and false negatives (FN).\n",
        "\n",
        "By analyzing these evaluation metrics, we can assess the overall performance of the logistic regression model in terms of its accuracy, precision, recall, F1 score, and ROC-AUC score. Additionally, the confusion matrix provides insights into the types of errors made by the model and helps in understanding its strengths and weaknesses."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix,classification_report,roc_auc_score,roc_curve\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Define a function to plot confusion matrix\n",
        "def plot_confusion_matrix(y_true,y_pred):\n",
        "    cm = confusion_matrix(y_true,y_pred)\n",
        "    sns.heatmap(cm, annot=True, cmap='Blues', fmt='d')\n",
        "    plt.xlabel('Predicted Labels')\n",
        "    plt.ylabel('True Labels')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Predict probabilities for test data\n",
        "y_pred_prob = model.predict_proba(X_test)\n",
        "\n",
        "# Define a function to plot ROC curve for multiclass classification\n",
        "def plot_roc_curve_multiclass(y_true, y_pred_prob, classes):\n",
        "    # Binarize the labels\n",
        "    y_true_binarized = label_binarize(y_true, classes=classes)\n",
        "\n",
        "    # Compute ROC Curve and ROC area for each class\n",
        "    fpr= dict()\n",
        "    tpr = dict()\n",
        "    roc_auc= dict()\n",
        "    for i in range(len(classes)):\n",
        "        fpr[i], tpr[i], _ = roc_curve(y_true_binarized[:, i], y_pred_prob[:, i])\n",
        "        roc_auc[i] = roc_auc_score(y_true_binarized[:, i], y_pred_prob[:, i])\n",
        "\n",
        "    # Plot ROC curve for each class\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    for i in range(len(classes)):\n",
        "        plt.plot(fpr[i], tpr[i],label='ROC curve  (area = {:.2f}) for class {}'.format(roc_auc[i], classes[i]))\n",
        "\n",
        "        # Plot random guesing line\n",
        "        plt.plot([0,1],[0,1], 'k--', label='Random Guessing')\n",
        "\n",
        "        # Set plot labels and title\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title('ROC Curve for Multiclass Classification')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "# Convert y_pred_prob to numpy array and reshape if needed\n",
        "y_pred_prob_np = np.array(y_pred_prob)\n",
        "if len(y_pred_prob_np.shape) ==1:\n",
        "    y_pred_prob_np = y_pred_prob_np.reshape(-1, 1)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plot_confusion_matrix(y_test,y_pred)\n",
        "\n",
        "# Plot ROC Curve for Multiclass Classification\n",
        "plot_roc_curve_multiclass(y_test, y_pred_prob_np, classes=[0,1,2])\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with Hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score,classification_report,precision_score,recall_score,f1_score\n",
        "\n",
        "# Define the Hyperparameter grid for logistic regression\n",
        "param_grid = {\n",
        "    'C': [0.1, 1.0, 10.0],\n",
        "    'penalty': ['l2']\n",
        "}\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Initialize LogisticRegression model with a different solver\n",
        "logistic_model = LogisticRegression( max_iter=1000)\n",
        "logistic_model.fit(X_train_scaled, y_train_resampled)\n",
        "\n",
        "\n",
        "# Initialize GridSearchCV with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(estimator= logistic_model, param_grid=param_grid, cv=5, scoring='accuracy', refit=True)\n",
        "\n",
        "# Fit the Algorithm\n",
        "grid_search.fit(X_train_scaled, y_train_resampled)\n",
        "\n",
        "# Get the best Hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best Hyperparameters:\",  best_params)\n",
        "\n",
        "# Train the model using the Best Hyperparameters\n",
        "best_model = LogisticRegression(**best_params)\n",
        "best_model.fit(X_train_scaled, y_train_resampled)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred = best_model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Classification Report\n",
        "print(\"Classification Report with Best Hyperparameters:\")\n",
        "print(classification_report(y_test,y_pred))\n",
        "\n",
        "# Precision Score\n",
        "print(\"Precision Score with Best Hyperparameters:\")\n",
        "print(precision_score(y_test,y_pred,average='weighted'))\n",
        "\n",
        "# Recall Score\n",
        "print(\"Recall Score with Best Hyperparameters:\")\n",
        "print(recall_score(y_test,y_pred,average='weighted'))\n",
        "\n",
        "# F1 Score\n",
        "print(\"F1 Score with Best Hyperparameters:\")\n",
        "print(f1_score(y_test,y_pred ,average='weighted'))\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "In the provided code snippet, I used Grid Search CV for hyperparameter optimization. Here's why:\n",
        "\n",
        "**Grid** **Search** **CV** (**Cross**-**Validation**): This technique exhaustively searches through a specified grid of hyperparameters for the best model performance. It evaluates each combination of hyperparameters using cross-validation and selects the one with the highest cross-validated score. Grid Search CV is straightforward to implement and provides a systematic approach to hyperparameter tuning.\n",
        "\n",
        "**Why** **Grid** **Search**?: Grid Search CV is particularly suitable when the hyperparameter space is not too large, as it evaluates all possible combinations. It ensures that we find the optimal hyperparameters within the specified grid, thereby potentially improving model performance.\n",
        "\n",
        "**Cross**-**Validation**: By using cross-validation within Grid Search CV, we ensure that the hyperparameters are tuned based on a more robust estimate of model performance. This helps to mitigate the risk of overfitting to the training data.\n",
        "\n",
        "Overall, Grid Search CV is a widely used and effective technique for hyperparameter optimization, making it a suitable choice for tuning logistic regression models in this scenario.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "After hyperparameter tuning using GridSearchCV, there doesn't appear to be a significant improvement in the evaluation metrics compared to the initial model. Here's a comparison:\n",
        "\n",
        "Before Hyperparameter Tuning (Initial Model):\n",
        "\n",
        "Accuracy: 0.5067\n",
        "\n",
        "Precision (weighted avg): 0.4904\n",
        "\n",
        "Recall (weighted avg): 0.5067\n",
        "\n",
        "F1 Score (weighted avg): 0.4890\n",
        "\n",
        "After Hyperparameter Tuning (GridSearchCV):\n",
        "\n",
        "Accuracy: 0.5055\n",
        "\n",
        "Precision (weighted avg): 0.4886\n",
        "\n",
        "Recall (weighted avg): 0.5055\n",
        "\n",
        "F1 Score (weighted avg): 0.4869\n",
        "\n",
        "As we can see, there is a very slight decrease in all the evaluation metrics after hyperparameter tuning. However, the changes are minimal and not significant. Therefore, it can be said that there is no noticeable improvement in the model's performance after hyperparameter tuning in this case.\n",
        "\n",
        "It's important to note that hyperparameter tuning may not always lead to improvements, and it's essential to carefully analyze the results to understand the impact of different hyperparameters on the model's performance. Additionally, further experimentation with different techniques and model architectures may be necessary to achieve better results.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### ML Model - 2\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model 2 Implementation\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report,precision_score,recall_score,f1_score\n",
        "\n",
        "\n",
        "\n",
        "print(X_train_scaled.shape)\n",
        "print(X_test_scaled.shape)\n",
        "print(y_train_resampled.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "# Initialize the Random Forest Classifier\n",
        "random_forest_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Fit the Random Forest Classifier on the training data\n",
        "random_forest_model.fit(X_train_scaled, y_train_resampled)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred_rf = random_forest_model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "print(\"Random Forest Classifier Accuracy:\", accuracy_rf)\n",
        "\n",
        "# Compute Confusion Matrix\n",
        "conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)\n",
        "print(\"Confusion Matrix for Random Forest Classifier:\")\n",
        "print(conf_matrix_rf)\n",
        "\n",
        "# Compute Classification report\n",
        "class_report_rf = classification_report(y_test, y_pred_rf)\n",
        "print(\"Classification Report for Random Forest Classifier:\")\n",
        "print(class_report_rf)\n",
        "\n",
        "# Compute Precision score\n",
        "prec_score_rf = precision_score(y_test, y_pred_rf, average='weighted')\n",
        "print(\"Precision Score for Random Forest Classifier:\")\n",
        "print(prec_score_rf)\n",
        "\n",
        "# Compute Recall Score\n",
        "recall_score_rf = recall_score(y_test, y_pred_rf, average='weighted')\n",
        "print(\"Recall Score for Random Forest Classifier:\")\n",
        "print(recall_score_rf)\n",
        "\n",
        "# Compute F1 Score\n",
        "f1_score_rf = f1_score(y_test, y_pred_rf, average='weighted')\n",
        "print(\"F1 Score for Random Forest Classifier:\")\n",
        "print(f1_score_rf)"
      ],
      "metadata": {
        "id": "8NlB1S85-kG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "The ML model used in this implementation is the Random Forest Classifier. Random Forest is an ensemble learning method that constructs a multitude of decision trees during training and outputs the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. It combines multiple decision trees to reduce overfitting and improve accuracy.\n",
        "\n",
        "Here's a summary of the model's performance using evaluation metric score charts:\n",
        "\n",
        "Accuracy: The accuracy score indicates the proportion of correctly classified instances out of the total instances. For the Random Forest Classifier, the accuracy score achieved on the test dataset is provided.\n",
        "\n",
        "Confusion Matrix: The confusion matrix is a table that is often used to describe the performance of a classification model on a set of test data for which the true values are known. It helps visualize the performance of the algorithm. The confusion matrix for the Random Forest Classifier is displayed.\n",
        "\n",
        "Classification Report: The classification report provides a summary of various metrics such as precision, recall, and F1-score for each class in the dataset. It also includes the overall metrics like macro-average, micro-average, and weighted average. The classification report for the Random Forest Classifier is presented.\n",
        "\n",
        "Precision, Recall, and F1 Score: These are commonly used evaluation metrics for classification problems. Precision measures the proportion of true positive predictions among all positive predictions. Recall measures the proportion of true positive predictions among all actual positive instances. F1 Score is the harmonic mean of precision and recall. For the Random Forest Classifier, precision, recall, and F1 score are calculated and provided.\n",
        "\n",
        "These evaluation metric score charts collectively provide insights into the performance of the Random Forest Classifier in terms of its accuracy and its ability to correctly classify instances across different classes. They help in understanding the strengths and weaknesses of the model and can guide further improvements if necessary.\n"
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import label_binarize\n",
        "# Plot Confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix_rf, annot=True, cmap='Blues', fmt='d', xticklabels=[\"Class 0\", \"Class 1\", \"Class 2\"], yticklabels=[\"Class 0\", \"Class 1\", \"Class 2\"])\n",
        "plt.title(\"Confusion Matrix for Random Forest Classifier\")\n",
        "plt.xlabel(\"Predicted labels\")\n",
        "plt.ylabel(\"True labels\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Binarize the target variable\n",
        "y_test_binarized = label_binarize(y_test, classes=[0,1,2])\n",
        "\n",
        "# Predict probabilities for test data\n",
        "y_pred_prob_rf = random_forest_model.predict_proba(X_test_scaled)\n",
        "\n",
        "# Plot ROC Curve for multiclass classification\n",
        "plt.figure(figsize=(8, 6))\n",
        "for i in range(3):\n",
        "    fpr, tpr, _ = roc_curve( y_test_binarized[:, i], y_pred_prob_rf[:, i])\n",
        "    plt.plot(fpr, tpr, label=f'class{i}) vs Rest')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Guessing')\n",
        "plt.title(\"ROC Curve for Random Forest Classifier(Multiclass)\")\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Define the Hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Initialize the Random Forest Classifier\n",
        "random_forest_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Initialize GridSearchCV with five fold cross validation\n",
        "grid_search_rf = GridSearchCV(estimator=random_forest_model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit the Algorithm\n",
        "grid_search_rf.fit(X_train_scaled, y_train_resampled)\n",
        "\n",
        "# Get the best Hyperparameters\n",
        "best_params_rf= grid_search_rf.best_params_\n",
        "print(\"Best Hyperparameters:\", best_params_rf)\n",
        "\n",
        "# Train the model  using the best hyperparameters\n",
        "best_rf_model = RandomForestClassifier(**best_params_rf, random_state=42)\n",
        "best_rf_model.fit(X_train_scaled, y_train_resampled)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_rf_tuned = best_rf_model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the performance of the Optimized model\n",
        "accuracy_best = accuracy_score(y_test, y_pred_rf_tuned)\n",
        "print(\"Accuracy with best Hyperparameters:\", accuracy_best)\n",
        "\n",
        "conf_matrix_best = confusion_matrix(y_test, y_pred_rf_tuned)\n",
        "print(\"Confusion Matrix with Best Hyperparameters:\",conf_matrix_best)\n",
        "\n",
        "class_report_best = classification_report(y_test, y_pred_rf_tuned)\n",
        "print(\"Classification Report with Best Hyperparameters:\", class_report_best)\n",
        "\n",
        "prec_score_best = precision_score(y_test, y_pred_rf_tuned, average='weighted')\n",
        "print(\"Precision Score with Best Hyperparameters:\", prec_score_best)\n",
        "\n",
        "recall_score_best = recall_score(y_test, y_pred_rf_tuned, average='weighted')\n",
        "print(\"Recall Score with Best Hyperparameters:\", recall_score_best)\n",
        "\n",
        "f1_score_best = f1_score(y_test, y_pred_rf_tuned, average='weighted')\n",
        "print(\"F1 Score with Best Hyperparameters:\", f1_score_best)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "In the provided code, I used GridSearchCV for hyperparameter optimization.\n",
        "\n",
        "GridSearchCV is a commonly used technique for hyperparameter optimization in machine learning. It exhaustively searches through a specified grid of hyperparameters and evaluates the model performance using cross-validation on each combination of hyperparameters.\n",
        "\n",
        "I chose GridSearchCV because:\n",
        "\n",
        "It systematically explores the entire search space of hyperparameters.\n",
        "\n",
        "It allows fine-grained control over the search space through the specification of parameter grids.\n",
        "\n",
        "It performs cross-validation during the search, providing reliable estimates of model performance.\n",
        "\n",
        "It is widely used and well-supported in the scikit-learn library, making it easy to implement and integrate into existing workflows.\n",
        "\n",
        "While GridSearchCV can be computationally expensive, especially with large search spaces or complex models, it provides a straightforward and systematic approach to hyperparameter tuning, which can lead to improved model performance.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Answer* Here.\n",
        "\n",
        "There wasn't a noticeable improvement in the Random Forest Classifier's performance after hyperparameter tuning. The accuracy, precision, recall, and F1 scores remained almost the same before and after tuning:\n",
        "\n",
        "Before Hyperparameter Tuning:\n",
        "\n",
        "Accuracy: 0.8829\n",
        "\n",
        "Precision (weighted avg): 0.8881\n",
        "\n",
        "Recall (weighted avg): 0.8829\n",
        "\n",
        "F1 Score (weighted avg): 0.8829\n",
        "\n",
        "After Hyperparameter Tuning:\n",
        "\n",
        "Accuracy: 0.8829\n",
        "\n",
        "Precision (weighted avg): 0.8881\n",
        "\n",
        "Recall (weighted avg): 0.8829\n",
        "\n",
        "F1 Score (weighted avg): 0.8829\n",
        "\n",
        "In summary, there was no improvement in performance metrics after hyperparameter tuning. However, the model's performance remained consistent, indicating that the default hyperparameters already provided near-optimal results for this dataset. Further optimization efforts may require experimenting with different algorithms or preprocessing techniques.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Sure, let's discuss the interpretation and business implications of each evaluation metric commonly used in classification tasks:\n",
        "\n",
        "**Accuracy**:\n",
        "\n",
        "Interpretation: Accuracy represents the proportion of correctly classified instances out of the total instances. It measures the overall correctness of the model across all classes.\n",
        "\n",
        "Business Implication: Higher accuracy indicates that the model is making fewer mistakes in classifying emails into their respective categories. This means that the model is effectively identifying the majority of emails correctly, which can lead to improved efficiency in email processing and decision-making.\n",
        "\n",
        "**Precision**:\n",
        "\n",
        "Interpretation: Precision measures the proportion of true positive predictions out of all positive predictions made by the model. It focuses on the accuracy of positive predictions.\n",
        "\n",
        "Business Implication: Precision is particularly important when the cost of false positives is high. In the context of email classification, high precision means that when the model predicts an email as, for example, spam, it is more likely to be correct. This can help reduce the risk of important emails being misclassified and overlooked.\n",
        "\n",
        "**Recall** (**Sensitivity**):\n",
        "\n",
        "Interpretation: Recall measures the proportion of true positive predictions out of all actual positive instances in the dataset. It focuses on the model's ability to capture all positive instances.\n",
        "\n",
        "Business Implication: Recall is crucial when the cost of false negatives is high. In email classification, high recall means that the model is effectively capturing the majority of relevant emails, minimizing the risk of important emails being missed or falsely classified as spam.\n",
        "\n",
        "**F1** **Score**:\n",
        "\n",
        "Interpretation: F1 score is the harmonic mean of precision and recall. It provides a balance between precision and recall and is useful when there is an uneven class distribution.\n",
        "\n",
        "Business Implication: F1 score provides a single metric that considers both false positives and false negatives. It is valuable when there is a trade-off between precision and recall, ensuring that the model maintains a balance between correctly classifying positive instances and capturing all positive instances.\n",
        "\n",
        "**Classification** **Report**:\n",
        "\n",
        "Interpretation: The classification report provides a comprehensive summary of various evaluation metrics (precision, recall, F1-score, and support) for each class in the dataset.\n",
        "\n",
        "Business Implication: A detailed classification report helps stakeholders understand the model's performance across different classes. It allows businesses to identify areas of improvement and focus on specific classes that may require further attention or optimization.\n",
        "\n",
        "In summary, each evaluation metric provides valuable insights into different aspects of the model's performance and its impact on business operations. By understanding these metrics and their implications, businesses can make informed decisions about the deployment and optimization of machine learning models in real-world applications."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "For positive business impact, the evaluation metrics that are particularly important depend on the specific objectives and requirements of the business task at hand. However, generally speaking, the following evaluation metrics are commonly considered for positive business impact and why:\n",
        "\n",
        "**Precision**:\n",
        "\n",
        "Importance: Precision measures the accuracy of positive predictions made by the model. It is crucial when the cost of false positives is high, and the business wants to minimize the risk of incorrect predictions in positive instances.\n",
        "\n",
        "Business Impact: High precision ensures that when the model predicts a positive outcome, it is more likely to be correct. For example, in email classification, high precision means that emails classified as spam are more likely to be actual spam, reducing the chances of important emails being mistakenly marked as spam and overlooked.\n",
        "\n",
        "**Recall** (**Sensitivity**):\n",
        "\n",
        "Importance: Recall measures the model's ability to capture all positive instances in the dataset. It is critical when the cost of false negatives is high, and the business wants to minimize the risk of missing positive instances.\n",
        "\n",
        "Business Impact: High recall ensures that the model effectively captures the majority of relevant instances. In email classification, high recall means that the model identifies most of the important emails, reducing the risk of critical emails being missed or falsely classified as spam.\n",
        "\n",
        "**F1** **Score**:\n",
        "\n",
        "Importance: F1 score provides a balance between precision and recall, making it valuable when there is a trade-off between the two metrics. It is useful in scenarios with uneven class distributions.\n",
        "\n",
        "Business Impact: F1 score ensures that the model maintains a balance between correctly identifying positive instances and capturing all positive instances. It helps optimize the overall performance of the model by considering both false positives and false negatives.\n",
        "\n",
        "By considering precision, recall, and F1 score, businesses can ensure that their machine learning models effectively meet their objectives while minimizing the potential negative impacts, such as misclassifications or missed opportunities. These metrics provide a comprehensive understanding of the model's performance and its implications for business operations, ultimately contributing to positive business outcomes.\n"
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kqTQ2AK1Ngjn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Based on the evaluation of the models, the Random Forest Classifier was chosen as the final prediction model. Here are the reasons:\n",
        "\n",
        "Higher Accuracy: The Random Forest Classifier achieved an accuracy of 0.8829 on the test set, which was the highest among the models evaluated.\n",
        "\n",
        "Balanced Performance: The Random Forest Classifier demonstrated balanced performance across precision, recall, and F1-score metrics for all classes, indicating its effectiveness in classifying the data.\n",
        "\n",
        "Robustness: Random forests are known for their robustness to overfitting and noise in data. By aggregating the predictions of multiple decision trees, random forests can handle complex datasets effectively.\n",
        "\n",
        "Feature Importance: Random forests provide feature importance scores, which can help in understanding the importance of different features in making predictions.\n",
        "\n",
        "Flexibility: Random forests can handle both classification and regression tasks, making them versatile for various types of predictive modeling tasks.\n",
        "\n",
        "Overall, the Random Forest Classifier offered a combination of high accuracy, balanced performance, robustness, and flexibility, making it the preferred choice as the final prediction model for this dataset.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "To explain the Random Forest Classifier model and feature importance, we can utilize the SHAP (SHapley Additive exPlanations) library, which provides a unified approach to explain the output of any machine learning model. SHAP values offer insights into the impact of each feature on the model's output for individual predictions.\n",
        "\n",
        "Here is how we can use SHAP to explain he Random Forest Classifier model:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n",
        "\n",
        "Since the performance metrics of Random Forest Classifier model are better  compared to that of Logistic Regression model,we  choosed to save Random Forest Clasifier Model for demonstration purposes:\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File\n",
        "import joblib\n",
        "from joblib import dump\n",
        "\n",
        "# Save the model to a file using joblib\n",
        "dump(best_rf_model,'best_model.joblib')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict the unseen data\n",
        "from joblib import load\n",
        "\n",
        "# Load the model from the file\n",
        "loaded_model = load('best_model.joblib')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the conclusion here.\n",
        "\n",
        "In this project, our objective was to build a machine learning model to predict the status of emails, aiming to enhance email management efficiency by automating email classification. This would facilitate prioritization of responses and optimization of workflow.\n",
        "\n",
        "The project began with a comprehensive understanding of the problem statement: automating email classification based on various features. Through exploratory data analysis (EDA), we gained insights into the dataset's characteristics, including numerical and categorical features, such as email content-related attributes and metadata.\n",
        "\n",
        "Our preprocessing pipeline involved handling missing values, scaling numerical features, and encoding categorical variables. Techniques like imputation and standardization were employed to ensure the data's suitability for training machine learning models.\n",
        "\n",
        "We implemented two machine learning models: Logistic Regression and Random Forest Classifier. Both models underwent hyperparameter optimization using GridSearchCV to enhance their performance. Evaluation was conducted using metrics like accuracy, precision, recall, and F1-score.\n",
        "\n",
        "Based on the evaluation results, the Logistic Regression model was quiet less compared to the Random Forest Classifier in terms of accuracy and other evaluation metrics. Therefore, we selected the Random Forest Classifier model as the final prediction model for deployment.\n",
        "\n",
        "The final model was saved using the Joblib library for deployment purposes. We also conducted tests on unseen data to validate the model's robustness and effectiveness in real-world scenarios.\n",
        "\n",
        "In conclusion, this project showcases the application of machine learning techniques in automating email classification tasks, leading to improved productivity and efficiency in email management workflows. Future work may involve further refinement of the model and integration into email client applications for seamless workflow integration."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}